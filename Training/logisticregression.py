# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxVQZMLrJnCoFYhBSFZbg8ePx24Bel8m
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
#import scikitplot as skplt
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score,precision_recall_fscore_support

data = pd.read_csv('/kaggle/input/ai-vs-human-text/AI_Human.csv')

data

# Count total entries
total_entries = len(data['generated'])

# Count AI-generated text
ai_generated_count = data['generated'].eq(1.0).sum()

# Count human-generated text
human_generated_count = data['generated'].eq(0.0).sum()

# Print the results
print("Total :", total_entries)
print("Total text by AI :", ai_generated_count)
print("Total text by Human :", human_generated_count)

data['generated'].value_counts()

percentage_ai = (text_by_ai / total_text) * 100
percentage_human = (text_by_human / total_text) * 100

print("Percentage of text by AI:", round(percentage_ai, 2), "%")
print("Percentage of text by Human:", round(percentage_human, 2), "%")

import matplotlib.pyplot as plt

categories = ['AI', 'Human']
percentages = [percentage_ai, percentage_human]

# Create bar chart
plt.bar(categories, percentages, color=['blue', 'green'])
plt.ylabel('Percentage')
plt.title('Distribution of Text by Source')
plt.ylim(0, 100)
plt.show()

data.info()

print("Summary Statistics for 'generated' column:")
print(data['generated'].describe())

plt.figure(figsize=(10, 6))
sns.histplot(data['generated'], bins=20, kde=True)
plt.title("Distribution of Generated Scores")
plt.xlabel("Generated Score")
plt.ylabel("Frequency")
plt.show()

data['text_length'] = data['text'].apply(len)
plt.figure(figsize=(10, 6))
sns.histplot(data['text_length'], bins=20, kde=True)
plt.title("Distribution of Text Lengths")
plt.xlabel("Text Length")
plt.ylabel("Frequency")
plt.show()

correlation = data[['text_length', 'generated']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

# Preprocessing: Split data into features (X) and target labels (y)
X = data['text']
y = data['generated']  # Assuming 'generated' column contains sentiment labels (1 for AI, 0 for Human)

tfidf_vectorizer = TfidfVectorizer(max_features=10000)
X_tfidf = tfidf_vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, stratify = y,
                                                    random_state=42)

lr_classifier = LogisticRegression()
lr_classifier.fit(X_train, y_train)

y_pred = lr_classifier.predict(X_test)
lr_accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", lr_accuracy)

precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)
plt.show()

