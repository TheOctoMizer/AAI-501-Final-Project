# -*- coding: utf-8 -*-
"""DistilRoBERTa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GD0sSnXP7LxBo7id-_FTi-6yp8XY-2oq

# Importing libraries
"""

import torch.nn as nn
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torchtext
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import Dataset, DataLoader, TensorDataset
from collections import Counter
from nltk.tokenize import word_tokenize
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from tokenizers import Tokenizer, pre_tokenizers, trainers
from tokenizers.models import WordLevel
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import WordLevelTrainer
import os
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, RobertaModel
from datasets import load_dataset
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

"""# Loading Data and Analysis"""

def set_seed(seed=123):
    '''Sets the seed of the entire notebook so results are the same every time we run.
    This is for REPRODUCIBILITY.'''
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed()

csv_path = '/kaggle/input/llm-detect-ai-generated-text/train_essays.csv'
csv_path1 = '/kaggle/input/ai-vs-human-text/AI_Human.csv'
csv_path3 = '/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv'
csv_path4 = '/kaggle/input/human-vs-llm-text-corpus/data.csv'
data_df = pd.read_csv(csv_path)
data_df1 = pd.read_csv(csv_path1)
data_df3 = pd.read_csv(csv_path3)
data_df4 = pd.read_csv(csv_path4)

data_df.head(2)

#1 = AI
#0 = Human

data_df1.head(2)

data_df3.head(2)

data_df4['source'] = data_df4['source'].apply(lambda x: 0 if x == 'Human' else 1)
data_df4.head(2)

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data_df = data_df.drop(columns=data_df.columns[[0, 1]])
new_column_names = ['text', 'label']
data_df.columns = new_column_names
data_df1.columns = new_column_names
data_df3 = data_df3.drop(columns=data_df3.columns[[2, 3, 4]])
data_df4 = data_df4.drop(columns=data_df4.columns[[2, 3, 4]])
data_df4.columns = new_column_names
cols = list(data_df3.columns)
text, label = cols.index('text'), cols.index('label')
cols[text], cols[label] = cols[label], cols[text]
data_df3 = data_df3[cols]
print(f"Dataset 1 length: {len(data_df)}")
print(f"Dataset 2 length: {len(data_df1)}")
print(f"Dataset 3 length: {len(data_df3)}")
print(f"Dataset 4 length: {len(data_df4)}")

"""To increase the size of the training data, I concatinated 3 datasets together. However because of hardware constraints I chose to use 100,000 images instead of the whole dataset"""

data_df = pd.concat([data_df, data_df3, data_df1, data_df4], axis=0)
data_df.dropna(inplace = True)
data_df.drop_duplicates(inplace = True)
data_df = data_df.sample(frac=1)
data_df = data_df[:100000]
data_df = data_df.reset_index(drop=True)
# data_df1
# data_df4

data_df.head()

data_df = data_df[data_df['label'] != 2]
value_count = data_df['label'].value_counts()
print(value_count)
plt.bar(['Student', 'AI'], list(value_count), color='teal')
plt.xlabel("LLM or Not",)
plt.ylabel('Count')
plt.title('Class Counts')
plt.show()
print(list(value_count))

"""# Data Pre-processing"""

def clean_text(text, stem=True):
    text = re.sub(r'[^A-Za-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.lower()
    return text

data_df['text'] = data_df['text'].apply(clean_text)

BATCH_SIZE = 16
MAX_LEN = 512

"""# Tokenization"""

import multiprocessing
import numpy as np
from tokenizers import Tokenizer, pre_tokenizers, trainers
from functools import partial

start = time.time()
# Example vocab size
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')


def tokenize_and_pad(sentence, tokenizer, max_length):
    encoded = tokenizer.encode_plus(
        sentence,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='np'
    )
    return {"input_ids": encoded['input_ids'].squeeze(),
           "attention_masks": encoded["attention_mask"].squeeze()}

# Function for parallel processing
def tokenize_and_pad_parallel(data, tokenizer, max_length):
    with multiprocessing.Pool() as pool:
        tokenized = pool.map(partial(tokenize_and_pad, tokenizer=tokenizer, max_length=max_length), data)
    return tokenized

# Apply tokenization and padding to all sentences in parallel
data_df['tokens'] = tokenize_and_pad_parallel(data_df['text'], tokenizer, MAX_LEN)

print(data_df['tokens'][0])
print(data_df['label'][:4])

end = time.time()
print(f"time taken: {end-start}")

vocab = tokenizer.get_vocab()
VOCAB_SIZE = len(vocab)
print(VOCAB_SIZE)

print(data_df.head(1))

from sklearn.model_selection import train_test_split

X = data_df['tokens'].values.tolist()
Y = data_df['label'].values

train_df, test_df = train_test_split(data_df, test_size=0.1, shuffle=True, random_state = 123)
train_df = train_df.reset_index()
test_df = test_df.reset_index()
print(train_df)
print(test_df)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe):
        self.labels = dataframe['label'].values
        self.tokens = dataframe['tokens'].values


    def __getitem__(self, idx):
        item = {}
        item['input_ids'] = torch.tensor(self.tokens[idx]['input_ids'])
        item['attention_masks'] = torch.tensor(self.tokens[idx]['attention_masks'])
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TextDataset(train_df)
test_dataset = TextDataset(test_df)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE * torch.cuda.device_count(), shuffle=True, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""# Loading Distil Bert for Fine-Tuning"""

device = ('cuda'
          if torch.cuda.is_available()
          else "mps"
          if torch.backends.mps.is_available()
          else "cpu"
          )

model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=1)
# model = nn.DataParallel(model)
model = model.to(device)
print(model)

start = time.time()
with torch.no_grad():
        for batch, data in enumerate(train_loader):
            inputs = data['input_ids'].pin_memory().to(device, non_blocking=True)
            attention_masks = data['attention_masks'].pin_memory().to(device, non_blocking=True)
            labels = data['labels'].pin_memory().to(device, non_blocking=True)
            pred = model(inputs, attention_mask = attention_masks)
            if batch == 1000:
                break
end = time.time()
print(end-start)

loss_func = torch.nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

def get_accuracy(pred,label):
    pred = (pred > 0.5).long()
    return torch.sum(pred == label).item()

CLIP = 1

def train(dataloader, model, loss_fn, optimizer, train_losses, train_acc):
    size = len(dataloader.dataset)
    model.train()
    loss_sum = 0
    accuracy = 0
    num_batches = len(dataloader)
    start = time.time()
    for batch, data in enumerate(dataloader):
        inputs = data['input_ids'].pin_memory().to(device, non_blocking=True)
        attention_masks = data['attention_masks'].pin_memory().to(device, non_blocking=True)
        labels = data['labels'].pin_memory().to(device, non_blocking=True)
        y = labels.float()
        pred = model(inputs, attention_mask = attention_masks)
        pred = nn.Sigmoid()(pred.logits)
        loss = loss_fn(pred.squeeze(), y)
        loss_sum += loss.item()
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        accuracy += get_accuracy(pred.squeeze(), y)
        if batch % 100 == 0:
            end = time.time()
            loss, current = loss.item(), (batch + 1) * len(inputs)
            print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]" + "                " + f"time taken: {(end-start):>5f} ")


    mean_loss = loss_sum / num_batches
    accuracy /= size
    train_losses.append(mean_loss)
    train_acc.append(accuracy)
    print(f"Training loss: {mean_loss:>7f}")
    print(f"Training accuracy: {(accuracy*100):>7f}")

def test(dataloader, model, loss_fn, test_losses, test_acc):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, accuracy = 0, 0
    with torch.no_grad():
        for batch, data in enumerate(dataloader):
            inputs = data['input_ids'].pin_memory().to(device, non_blocking=True)
            attention_masks = data['attention_masks'].pin_memory().to(device, non_blocking=True)
            labels = data['labels'].pin_memory().to(device, non_blocking=True)
            y = labels.float()
            pred = model(inputs, attention_mask = attention_masks)
            pred = nn.Sigmoid()(pred.logits)
            test_loss += loss_fn(pred.squeeze(), y).item()
            accuracy += get_accuracy(pred.squeeze(), y)
    test_loss /= num_batches
    accuracy /= size
    print("________")
    print(f"Test loss: {test_loss:>7f}")
    print(f"Test accuracy: {(accuracy*100):>0.1f}%")
    test_losses.append(test_loss)
    test_acc.append(accuracy)

torch.cuda.empty_cache()

"""# Training"""

train_losses = []
test_losses = []
test_acc = []
train_acc = []

for i in range(5):
    print(f'Epoch {i} -----------------------------------')
    train(train_loader, model, loss_func, optimizer, train_losses, train_acc)
    test(test_loader, model, loss_func, test_losses, test_acc)

print('Finished Training')

plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(test_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(train_acc, label='Training accuracy')
plt.plot(test_acc, label='Val accuracy')
plt.xlabel('Epoch')
plt.ylabel('accuracy')
plt.title('Training and Validation accuracy Over Epochs')
plt.legend()
plt.show()

test = pd.read_csv("/kaggle/input/llm-detect-ai-generated-text/test_essays.csv")
test.head()

"""# Evaluation"""

def processText(text):
    text = clean_text(text)
    text = tokenize_and_pad(text, tokenizer, MAX_LEN)
    text_tensor = torch.tensor(text["input_ids"], dtype=torch.int32)
    text_attention = torch.tensor(text["attention_masks"], dtype=torch.int32)
    return {"input_ids": text_tensor, "attention_mask": text_attention}

test['text'] = test['text'].apply(processText)

for data in train_loader:
    print(data['input_ids'].shape)
    break

for batch, (data) in enumerate(train_loader):
    print(data['input_ids'].shape)
    break

torch.cuda.empty_cache()

eval_loader = DataLoader(test['text'], batch_size=3, shuffle=True)

with torch.no_grad():
    for data in eval_loader:
        inputs = data['input_ids'].to(device)
        attention = data['attention_mask'].to(device)
        pred = model(inputs, attention_mask = attention)
        pred = nn.Sigmoid()(pred.logits)

test["generated"] = pred.cpu().detach().numpy().squeeze()

submission = test[["id", "generated"]]

submission.to_csv("submission.csv", index=False)

submission.head()

save_directory = './tokenizer_directory'
tokenizer.save_pretrained("./custom_tokenizer.json")

torch.cuda.empty_cache()

import numpy as np
from sklearn.metrics import confusion_matrix,classification_report
import seaborn as sns
import matplotlib.pyplot as plt

BATCH_SIZE = 32
preds = []
true_preds = []

with torch.no_grad():
    for data in test_loader:
        x = data['input_ids'].to(device)
        y = data['labels'].to(device)
        attention = data['attention_masks'].to(device)
        pred = model(x, attention_mask = attention)
        pred = nn.Sigmoid()(pred.logits)
        true_preds.append(y.cpu().detach())
        pred = (pred > 0.5).long()
        preds.append(pred.cpu().detach())

flattened_preds = [arr.flatten() for arr in preds]
preds = np.concatenate(flattened_preds)

flattened_true = [arr.flatten() for arr in true_preds]
true_preds= np.concatenate(flattened_true)


print(preds)
print(true_preds )


cm = confusion_matrix(true_preds, preds)
sns.heatmap(cm,
            annot=True,
            fmt='g',
            xticklabels=['Student', 'AI'],
            yticklabels=['Student', 'AI'])
plt.xlabel('Prediction',fontsize=13)
plt.ylabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)
plt.show()

print(classification_report(true_preds, preds, target_names=['student', 'ai']))